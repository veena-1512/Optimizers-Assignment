{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7434e60-7d8b-493a-8539-178599fe53be",
   "metadata": {},
   "source": [
    "Part 1: Uoder_taodiog Optimiaer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9db92e-5727-4271-bcf9-3e03b3b6130d",
   "metadata": {},
   "source": [
    "Q1. What is the role of optimization algorithms in artificial neural networks Why are they necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de0e46b-0d15-474c-992d-266004b2bef5",
   "metadata": {},
   "source": [
    "Optimization algorithms play a crucial role in training artificial neural networks (ANNs). These algorithms are necessary for adjusting the parameters of the network, such as weights and biases, to minimize the difference between the actual output of the network and the desired output (i.e., the error or loss function).\n",
    "\n",
    "Here's why they are necessary:-\n",
    "\n",
    "1. Parameter Adjustment: ANNs typically have a large number of parameters that define the connections between neurons. Optimization algorithms are necessary to adjust these parameters iteratively during the training process to minimize the error between the predicted and actual outputs.\n",
    "\n",
    "2. Gradient Descent: Most optimization algorithms used in training ANNs are variants of gradient descent. Gradient descent aims to find the minimum of a function (the loss function in this case) by iteratively moving in the direction of the steepest descent. This process involves computing the gradient of the loss function with respect to the parameters of the network and updating the parameters in the opposite direction of the gradient.\n",
    "\n",
    "3. Finding Global Optima: The loss function in ANNs is typically non-convex, meaning it can have multiple local minima. Optimization algorithms help in navigating this complex landscape to find a set of parameters that corresponds to a good solution. Although they may not guarantee finding the global optimum, they aim to converge to a satisfactory local minimum.\n",
    "\n",
    "4. Efficient Learning: Optimization algorithms are crucial for efficiently training ANNs. They enable the network to learn from data in a computationally feasible manner by iteratively adjusting parameters based on the observed errors.\n",
    "\n",
    "5. Regularization: Some optimization algorithms incorporate regularization techniques, such as L1 or L2 regularization, which help prevent overfitting by penalizing large parameter values. These techniques are essential for improving the generalization ability of the network.\n",
    "\n",
    "6. Handling Non-linearities: ANNs often involve non-linear activation functions, which make the loss function non-convex. Optimization algorithms are necessary to deal with the complexities introduced by these non-linearities and to ensure effective learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28219abc-f672-4595-bba1-f23ada61440b",
   "metadata": {},
   "source": [
    "Q2. Explain the concept of gradient descent and its variants. Discuss their differences and tradeoffs in terms \n",
    "of convergence speed and memory requirements. \n",
    "\n",
    "Gradient descent is a fundamental optimization algorithm used to minimize a function, typically the loss function in the context of training neural networks. The basic idea is to iteratively move in the direction of the negative gradient of the function until a minimum is reached. This process can be mathematically represented as:\n",
    "\n",
    "θt+1=θt−α∇f(θt)\n",
    "\n",
    "Where:\n",
    "θt represents the parameters (weights and biases) of the neural network at iterationt.  \n",
    "\n",
    "α is the learning rate, which determines the size of the step taken in each iteration.\n",
    "\n",
    "∇f(θt) is the gradient of the function f with respect to the parameters θ at iteration t.\n",
    "\n",
    "Gradient descent variants differ in how they update the parameters and handle the learning rate. Some common variants include:\n",
    "\n",
    "1. Batch Gradient Descent (BGD):\n",
    "\n",
    "In BGD, the gradient is computed using the entire training dataset.\n",
    "This approach ensures a precise estimation of the gradient but can be computationally expensive, especially for large datasets, and may have memory requirements proportional to the dataset size.\n",
    "\n",
    "2. Stochastic Gradient Descent (SGD):\n",
    "\n",
    "SGD computes the gradient using only one randomly chosen training sample at each iteration.\n",
    "It offers faster updates and reduced memory requirements since it processes only one sample at a time.\n",
    "However, the updates may be noisy, leading to more oscillations in the optimization process.\n",
    "\n",
    "3. Mini-batch Gradient Descent:\n",
    "\n",
    "Mini-batch GD strikes a balance between BGD and SGD by computing the gradient using a small subset of the training data (a mini-batch).\n",
    "It combines the advantages of both BGD and SGD: reduced computational cost compared to BGD and smoother convergence compared to SGD.\n",
    "The choice of mini-batch size affects the tradeoff between convergence speed and memory requirements.\n",
    "\n",
    "4. Momentum:\n",
    "\n",
    "Momentum incorporates a momentum term that accelerates the parameter updates in the direction of persistent gradients and dampens oscillations.\n",
    "It helps overcome small local minima and accelerates convergence, especially in regions with high curvature.\n",
    "Momentum requires additional memory to store the momentum terms.\n",
    "\n",
    "5. Adagrad, RMSProp, and Adam:\n",
    "\n",
    "These adaptive optimization algorithms adjust the learning rate for each parameter based on the historical gradients.\n",
    "Adagrad adapts the learning rates individually for each parameter based on the accumulated gradients.\n",
    "RMSProp and Adam improve upon Adagrad by dynamically adjusting the learning rates using exponentially decaying averages of past gradients and squared gradients.\n",
    "These algorithms offer better convergence properties on non-convex optimization problems but may require more memory for storing additional parameters and computation of adaptive learning rates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75e5f49-cd5a-49a8-97c9-44e77a2bc734",
   "metadata": {},
   "source": [
    "Q4. Discuss the concepts of momentum and learning rate in the context of optimization algorithms. How do \n",
    "they impact convergence and model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e28b2f-7a06-4ab3-bf9d-02d00f521caa",
   "metadata": {},
   "source": [
    "In the context of optimization algorithms, momentum and learning rate are critical parameters that influence the convergence behavior and performance of the model during training.\n",
    "\n",
    "1. Momentum:\n",
    "\n",
    "Momentum is a technique used to accelerate gradient descent by adding a fraction of the previous update to the current update. It helps to smooth out the variations in the gradient descent process, especially when the optimization landscape has irregularities or noise. Mathematically, the update rule for momentum can be represented as:vt+1 =βvt+(1−β)∇f(θt).\n",
    " \n",
    " θt+1=θt−αvt+1\n",
    "\n",
    "Where:\n",
    "\n",
    "    vt is the momentum at iterationt.\n",
    "\n",
    "    β is the momentum parameter (usually between 0 and 1), determining how much of the previous update is retained.\n",
    "\n",
    "    ∇f(θt) is the gradient of the loss function with respect to the parameters at iteration t.\n",
    "\n",
    "    α is the learning rate.\n",
    "\n",
    "Momentum helps to speed up convergence by maintaining a consistent direction of movement, especially when the gradients keep changing direction frequently. It also helps in escaping from local minima and plateaus by allowing the optimizer to build up speed in the direction of the minimum gradient.\n",
    "\n",
    "2. Learning Rate:\n",
    "\n",
    "The learning rate is a hyperparameter that determines the size of the steps taken during optimization. It controls the rate at which the parameters of the model are updated in the direction of the gradient. Choosing an appropriate learning rate is crucial for the convergence and stability of the optimization process.\n",
    "\n",
    "If the learning rate is too small, the optimization process may converge very slowly, and it may take a long time for the model to reach a satisfactory solution. On the other hand, if the learning rate is too large, the optimization process may oscillate around the minimum or even diverge, leading to instability.\n",
    "\n",
    "Different learning rate schedules, such as fixed learning rates, adaptive learning rates (e.g., AdaGrad, RMSProp, Adam), or learning rate annealing, can be used to adjust the learning rate during training based on the behavior of the optimization process.\n",
    "\n",
    "\n",
    "Impact on Convergence and Model Performance:\n",
    "\n",
    " Momentum: Adding momentum generally accelerates convergence by smoothing out fluctuations in the gradient updates. It helps the optimizer to maintain a more consistent direction of movement towards the minimum. This can lead to faster convergence and improved model performance, especially in complex optimization landscapes. However, setting the momentum parameter too high can lead to overshooting or oscillations.\n",
    "\n",
    " Learning Rate: The learning rate directly affects the convergence speed and stability of the optimization process. A well-tuned learning rate is essential for achieving faster convergence without sacrificing stability. Too high a learning rate can lead to instability or divergence, while too low a learning rate can result in slow convergence. Adaptive learning rate methods dynamically adjust the learning rate based on the observed behavior of the optimization process, potentially leading to faster convergence and better model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d0f396-a42b-4c3c-bbe8-1a5bea22b89a",
   "metadata": {},
   "source": [
    "Part 2: Optimiaer Techoique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a64f03f-dc89-41d6-99ce-21e5dc2652aa",
   "metadata": {},
   "source": [
    "Q5. Explain the concept of Stochastic gradient Descent (SGD) and its advantages compared to traditional \n",
    "gradient descent. Discuss its limitations and scenarios where it is most suitable.\n",
    "\n",
    "Stochastic Gradient Descent (SGD) is a variant of the gradient descent optimization algorithm commonly used in training machine learning models, including neural networks. In SGD, instead of computing the gradient using the entire training dataset, the gradient is estimated using a single randomly chosen training example at each iteration. The basic update rule for SGD can be expressed as follows:\n",
    "θt+1=θt−α∇f(xi,θt)\n",
    "\n",
    "Where:\n",
    "\n",
    "    θt represents the parameters (weights and biases) of the model at iteration \n",
    "    \n",
    "    α is the learning rate.\n",
    "    ∇f(xi,θt)is the gradient of the loss function f with respect to the parameters θ computed using a randomly chosen training example \n",
    "    xi at iteration t.\n",
    "  \n",
    "  Advantages of Stochastic Gradient Descent (SGD):\n",
    "\n",
    "1. Efficiency: SGD is computationally efficient because it processes only one training example at a time, making it suitable for large datasets. By updating the parameters more frequently, SGD can reach convergence faster compared to traditional gradient descent.\n",
    "\n",
    "2. Regularization: The inherent noise introduced by using a single training example at each iteration acts as a form of regularization, which helps prevent overfitting. This stochastic nature can lead to better generalization performance, especially when the dataset is noisy or the model is prone to overfitting.\n",
    "\n",
    "3. Escape from Local Minima: The noisy updates in SGD enable the algorithm to escape shallow local minima and saddle points more easily compared to traditional gradient descent. This stochastic nature adds randomness to the optimization process, allowing the algorithm to explore a wider range of solutions.\n",
    "\n",
    "4. Online Learning: SGD is well-suited for online learning scenarios where data arrives sequentially or in batches. It allows the model to adapt quickly to changes.\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6e0076-7bc7-4924-ba38-4bf992c41f7f",
   "metadata": {},
   "source": [
    "Q6. Describe the concept of Adam optimizer and how it combines momentum and adaptive learning rates. \n",
    "Discuss its benefits and potential drawbacks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677cce52-0bbe-47ec-9c08-8380bbfd3428",
   "metadata": {},
   "source": [
    "RMSprop (Root Mean Square Propagation):\n",
    "\n",
    "Objective: RMSprop aims to improve upon the limitations of other optimization algorithms, particularly AdaGrad, which suffers from diminishing learning rates.\n",
    "\n",
    "Key Idea:\n",
    "\n",
    "Instead of accumulating all past squared gradients (as AdaGrad does), RMSprop restricts the accumulation to a fixed window of the most recent gradients.It computes a moving average of squared gradients to normalize the gradient itself.\n",
    "\n",
    "Adaptive Learning Rates:\n",
    "\n",
    "RMSprop dynamically adjusts the learning rate based on historical gradient magnitudes.\n",
    "\n",
    "This adaptability helps address challenges like slow convergence and oscillations.\n",
    "\n",
    "Strengths:\n",
    "\n",
    "Stability: RMSprop provides more stable convergence by preventing overly aggressive learning rate decay.\n",
    "\n",
    "Robustness: It works well across various problem domains and architectures.\n",
    "\n",
    "Weaknesses:\n",
    "\n",
    "Hyperparameter Sensitivity: The choice of hyperparameters (such as the decay rate) can significantly impact performance.\n",
    "\n",
    "Local Minima: Like other optimizers, RMSprop may still get stuck in local minima.\n",
    "\n",
    "2. Adam (Adaptive Moment Estimation):\n",
    "\n",
    "Objective: Adam combines the benefits of both RMSprop and momentum.\n",
    "\n",
    "Key Idea:\n",
    "\n",
    "It maintains both a moving average of gradients (like RMSprop) and a moving average of past gradients’ first moments (like momentum).\n",
    "\n",
    "These moments are used to adaptively adjust the learning rate\n",
    "\n",
    "Adaptive Learning Rates:\n",
    "\n",
    "Adam adapts the learning rate based on both gradient magnitudes and their first moments.\n",
    "\n",
    "Strengths:\n",
    "\n",
    "Efficiency: Adam often converges faster than RMSprop due to its momentum component.\n",
    "\n",
    "Robustness: It performs well across different tasks and architectures.\n",
    "\n",
    "Weaknesses:\n",
    "\n",
    "Hyperparameter Sensitivity: Like RMSprop, Adam’s performance depends on hyperparameter tuning.\n",
    "\n",
    "Memory Usage: The additional moving averages increase memory requirements.\n",
    "\n",
    "        Comparison:\n",
    "\n",
    "    Similarities:\n",
    "Both RMSprop and Adam use adaptive learning rates.\n",
    "\n",
    "They address the diminishing learning rate issue.\n",
    "\n",
    "    Differences:\n",
    "\n",
    "Momentum: Adam incorporates momentum, which can lead to faster convergence.\n",
    "\n",
    "Memory: Adam requires additional memory for maintaining moving averages.\n",
    "\n",
    "Performance: The choice between RMSprop and Adam depends on the specific problem and dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b435e3b-c99b-4cfd-9bec-d164e174528c",
   "metadata": {},
   "source": [
    "            Part 3: Applyiog Optimiaer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81036eb3-407e-48a0-80a5-f4c315d92b00",
   "metadata": {},
   "source": [
    "Q8. Implement SGD, Adam, and RMSprop optimizers in a deep learning model using a framework of your \n",
    "choice. Train the model on a suitable dataset and compare their impact on model convergence and \n",
    "performancen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6541978c-4ee1-4a64-af6e-e667160faba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.2.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2022.11.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.10/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.10/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (2.8.8)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.2.1)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtransforms\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dc6d6ba-39c9-4ab6-b245-2100321fd6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74cf1f9c-c881-4c53-aa86-9d8121c38cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f949954-b28d-4440-aa31-00aa5b9e9feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Tensorflow\n",
      "  Downloading tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard<2.16,>=2.15\n",
      "  Downloading tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from Tensorflow) (3.7.0)\n",
      "Collecting wrapt<1.15,>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from Tensorflow) (22.0)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0\n",
      "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ml-dtypes~=0.2.0\n",
      "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.60.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from Tensorflow) (1.16.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from Tensorflow) (4.21.11)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from Tensorflow) (65.5.1)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Collecting flatbuffers>=23.5.26\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting keras<2.16,>=2.15.0\n",
      "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from Tensorflow) (1.23.5)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from Tensorflow) (4.9.0)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.36.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->Tensorflow) (0.38.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->Tensorflow) (2.28.1)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.27.0-py2.py3-none-any.whl (186 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.8/186.8 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<2,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.5.2-py3-none-any.whl (103 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.9/103.9 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->Tensorflow) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->Tensorflow) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->Tensorflow) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->Tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->Tensorflow) (2.1.1)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->Tensorflow) (3.2.2)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, opt-einsum, ml-dtypes, markdown, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, rsa, requests-oauthlib, pyasn1-modules, google-auth, google-auth-oauthlib, tensorboard, Tensorflow\n",
      "Successfully installed Tensorflow-2.15.0.post1 absl-py-2.1.0 astunparse-1.6.3 cachetools-5.3.2 flatbuffers-23.5.26 gast-0.5.4 google-auth-2.27.0 google-auth-oauthlib-1.2.0 google-pasta-0.2.0 grpcio-1.60.1 keras-2.15.0 libclang-16.0.6 markdown-3.5.2 ml-dtypes-0.2.0 opt-einsum-3.3.0 pyasn1-0.5.1 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.15.1 tensorboard-data-server-0.7.2 tensorflow-estimator-2.15.0 tensorflow-io-gcs-filesystem-0.36.0 termcolor-2.4.0 werkzeug-3.0.1 wrapt-1.14.1\n"
     ]
    }
   ],
   "source": [
    "!pip install Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "684a99b3-f273-4989-9240-96d3c25ab930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.17.0-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: torch==2.2.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.2.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->torchvision) (3.1.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->torchvision) (1.11.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->torchvision) (4.9.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->torchvision) (3.13.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->torchvision) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->torchvision) (2.19.3)\n",
      "Requirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->torchvision) (2.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->torchvision) (2.8.8)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->torchvision) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->torchvision) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->torchvision) (11.4.5.107)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->torchvision) (2022.11.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->torchvision) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->torchvision) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0->torchvision) (12.3.101)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.2.0->torchvision) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.2.0->torchvision) (1.2.1)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7fc8e32-96b6-4b48-a866-29cfb5975cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 38038823.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 47917600.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1648877/1648877 [00:00<00:00, 12711119.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 8186733.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98551940-2966-43d8-a5ac-9873550ffbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define learning rates and hyperparameters\n",
    "learning_rate_sgd = 0.01\n",
    "learning_rate_adam = 0.001\n",
    "learning_rate_rmsprop = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "745e4ec0-bbd9-4706-a720-91d8f8355c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_sgd = optim.SGD(model.parameters(), lr=learning_rate_sgd)\n",
    "optimizer_adam = optim.Adam(model.parameters(), lr=learning_rate_adam)\n",
    "optimizer_rmsprop = optim.RMSprop(model.parameters(), lr=learning_rate_rmsprop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac56eaba-1388-44cc-ad04-697b3341994e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Module [SimpleNN] is missing the required \"forward\" function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m optimizer_adam\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     10\u001b[0m optimizer_rmsprop\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 12\u001b[0m outputs_sgd \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m outputs_adam \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     14\u001b[0m outputs_rmsprop \u001b[38;5;241m=\u001b[39m model(inputs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:374\u001b[0m, in \u001b[0;36m_forward_unimplemented\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_unimplemented\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    364\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Define the computation performed at every call.\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    Should be overridden by all subclasses.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m        registered hooks while the latter silently ignores them.\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 374\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] is missing the required \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m function\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Module [SimpleNN] is missing the required \"forward\" function"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss_sgd = 0.0\n",
    "    running_loss_adam = 0.0\n",
    "    running_loss_rmsprop = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer_sgd.zero_grad()\n",
    "        optimizer_adam.zero_grad()\n",
    "        optimizer_rmsprop.zero_grad()\n",
    "\n",
    "        outputs_sgd = model(inputs)\n",
    "        outputs_adam = model(inputs)\n",
    "        outputs_rmsprop = model(inputs)\n",
    "\n",
    "        loss_sgd = criterion(outputs_sgd, labels)\n",
    "        loss_adam = criterion(outputs_adam, labels)\n",
    "        loss_rmsprop = criterion(outputs_rmsprop, labels)\n",
    "\n",
    "        loss_sgd.backward()\n",
    "        loss_adam.backward()\n",
    "        loss_rmsprop.backward()\n",
    "\n",
    "        optimizer_sgd.step()\n",
    "        optimizer_adam.step()\n",
    "        optimizer_rmsprop.step()\n",
    "\n",
    "        running_loss_sgd += loss_sgd.item()\n",
    "        running_loss_adam += loss_adam.item()\n",
    "        running_loss_rmsprop += loss_rmsprop.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}:\")\n",
    "    print(f\"SGD Loss: {running_loss_sgd / len(trainloader)}\")\n",
    "    print(f\"Adam Loss: {running_loss_adam / len(trainloader)}\")\n",
    "    print(f\"RMSprop Loss: {running_loss_rmsprop / len(trainloader)}\")\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeda934-881d-461c-aba8-0ac53e6e1f1c",
   "metadata": {},
   "source": [
    "Q9.  Discuss the considerations and tradeoffs when choosing the appropriate optimizer for a given neural \n",
    "network architecture and task. consider factors such as convergence speed, stability, and \n",
    "generalization performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c7986d-37e1-47a9-8355-a170f70fd2d6",
   "metadata": {},
   "source": [
    "Choosing the appropriate optimizer for a given neural network architecture and task involves considering various factors such as convergence speed, stability, and generalization performance. Here are some key considerations and tradeoffs:\n",
    "\n",
    "1. Convergence Speed:\n",
    "\n",
    "Adam and RMSprop: Adaptive optimizers like Adam and RMSprop tend to converge faster than traditional optimizers like SGD, especially in the presence of sparse gradients or highly non-convex optimization landscapes. This is because they adapt the learning rates for each parameter based on the historical gradients.\n",
    "\n",
    "SGD: While SGD may converge more slowly than adaptive optimizers in some cases, it can still converge effectively, particularly with carefully tuned learning rates and momentum. Additionally, SGD may be more suitable for tasks with simple optimization landscapes.\n",
    "\n",
    "2. Stability:\n",
    "\n",
    "Adam and RMSprop: Adaptive optimizers often offer more stable optimization compared to traditional optimizers like SGD. They can handle varying gradients and learning rates, making them less sensitive to hyperparameter choices and fluctuations in the optimization process.\n",
    "\n",
    "SGD: SGD may suffer from oscillations or slow convergence if the learning rate is not appropriately tuned. However, with proper tuning and the use of techniques like momentum, SGD can achieve stable optimization results.\n",
    "\n",
    "Generalization Performance:\n",
    "\n",
    "SGD: SGD with momentum or learning rate schedules may improve generalization performance by preventing overfitting. By allowing the optimizer to escape sharp local minima and explore different regions of the optimization landscape, SGD can lead to better generalization to unseen data.\n",
    "Adam and RMSprop: While Adam and RMSprop often converge faster, there is a risk of overfitting, especially if the optimization process is too aggressive. Regularization techniques or early stopping may be necessary to prevent overfitting when using adaptive optimizers.\n",
    "Memory and Computational Efficiency:\n",
    "\n",
    "SGD: SGD typically requires less memory and computational resources compared to adaptive optimizers like Adam and RMSprop. This can be advantageous when training large-scale models or in resource-constrained environments.\n",
    "Adam and RMSprop: Adaptive optimizers may require more memory and computational resources due to the additional calculations involved in maintaining moment estimates and adapting learning rates. However, they may still offer faster convergence and better performance on certain tasks.\n",
    "Hyperparameter Sensitivity:\n",
    "\n",
    "Adam and RMSprop: Adam and RMSprop have fewer hyperparameters compared to SGD with momentum, making them easier to tune. However, the choice of hyperparameters such as the learning rate, decay rates (\n",
    "�\n",
    "β), and epsilon (\n",
    "�\n",
    "ϵ) can still impact performance and may require some experimentation.\n",
    "SGD: SGD with momentum may require more careful tuning of hyperparameters, such as the learning rate and momentum coefficient, to achieve optimal performance. However, once properly tuned, SGD can offer competitive results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a90ead-c983-4e5a-9e58-8caa6a03d1fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c6dede-aacb-4e73-8130-a7e96a357c67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
